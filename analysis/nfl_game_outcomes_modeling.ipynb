{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a34306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load core libraries for tabular modeling and evaluation.\n",
    "# We will train three targets in this notebook:\n",
    "# 1) spread (home margin)\n",
    "# 2) total points\n",
    "# 3) home win probability\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    accuracy_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1e1ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2760\n",
      "Columns: 36\n",
      "   season  week away_team home_team\n",
      "0    2016     1       CAR       DEN\n",
      "1    2016     1        TB       ATL\n",
      "2    2016     1       BUF       BAL\n"
     ]
    }
   ],
   "source": [
    "# Load the modeling dataset built in EDA.\n",
    "# This table contains game outcomes and engineered differential features.\n",
    "df = pd.read_parquet(\"../data/final_first_model.parquet\")\n",
    "df = df.iloc[:-1].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", len(df.columns))\n",
    "print(df[[\"season\", \"week\", \"away_team\", \"home_team\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b5a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 2191 | Test rows: 569\n"
     ]
    }
   ],
   "source": [
    "# Create a strict temporal split.\n",
    "# Train: seasons up to 2023\n",
    "# Test:  seasons 2024+\n",
    "train = df[df[\"season\"] <= 2023].copy()\n",
    "test = df[df[\"season\"] >= 2024].copy()\n",
    "\n",
    "print(\"Train rows:\", len(train), \"| Test rows:\", len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df259efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPREAD Ridge | MAE: 9.993 RMSE: 12.865\n",
      "SPREAD Baseline(0) | MAE: 11.186 RMSE: 14.447\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# SPREAD MODELING (home margin)\n",
    "# -----------------------------\n",
    "# Target: margin = home_score - away_score\n",
    "# Features: pregame differentials + division game flag\n",
    "feat_spread = [\"elo_diff\", \"off_diff\", \"def_allowed_diff\", \"to_diff\", \"rest_diff\", \"div_game\"]\n",
    "\n",
    "Xtr_s, ytr_s = train[feat_spread], train[\"margin\"]\n",
    "Xte_s, yte_s = test[feat_spread], test[\"margin\"]\n",
    "\n",
    "spread_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge(alpha=1.0)),\n",
    "])\n",
    "\n",
    "spread_model.fit(Xtr_s, ytr_s)\n",
    "pred_s = spread_model.predict(Xte_s)\n",
    "\n",
    "mae_s = mean_absolute_error(yte_s, pred_s)\n",
    "rmse_s = mean_squared_error(yte_s, pred_s) ** 0.5\n",
    "print(\"SPREAD Ridge | MAE:\", round(mae_s, 3), \"RMSE:\", round(rmse_s, 3))\n",
    "\n",
    "# Baseline for spread: always predict 0 margin.\n",
    "pred0 = np.zeros(len(test))\n",
    "print(\n",
    "    \"SPREAD Baseline(0) | MAE:\",\n",
    "    round(mean_absolute_error(yte_s, pred0), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_s, pred0) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Conclusion: Ridge improves spread error vs baseline (MAE 9.993 vs 11.186), so differential features add real signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289ca385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: {'ridge__alpha': 100}\n",
      "SPREAD Tuned Ridge | MAE: 9.99 RMSE: 12.871\n"
     ]
    }
   ],
   "source": [
    "# Tune Ridge alpha for spread using cross-validation on train only.\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge()),\n",
    "])\n",
    "param_grid = {\"ridge__alpha\": [0.1, 0.3, 1, 3, 10, 30, 100]}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid, scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "gs.fit(Xtr_s, ytr_s)\n",
    "\n",
    "spread_model = gs.best_estimator_\n",
    "pred_s_tuned = spread_model.predict(Xte_s)\n",
    "print(\"Best alpha:\", gs.best_params_)\n",
    "print(\n",
    "    \"SPREAD Tuned Ridge | MAE:\",\n",
    "    round(mean_absolute_error(yte_s, pred_s_tuned), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_s, pred_s_tuned) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Conclusion: Tuning alpha gives only marginal change (MAE 9.990), so base Ridge was already near-optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6db87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPREAD HGB | MAE: 10.249 RMSE: 13.114\n"
     ]
    }
   ],
   "source": [
    "# Optional nonlinear benchmark for spread.\n",
    "spread_hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=600,\n",
    "    min_samples_leaf=40,\n",
    "    l2_regularization=0.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "spread_hgb.fit(Xtr_s, ytr_s)\n",
    "pred_s_hgb = spread_hgb.predict(Xte_s)\n",
    "\n",
    "print(\n",
    "    \"SPREAD HGB | MAE:\",\n",
    "    round(mean_absolute_error(yte_s, pred_s_hgb), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_s, pred_s_hgb) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Conclusion: HGB underperforms Ridge for spread on this split, so linear structure generalizes better here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3d2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# TOTAL POINTS MODELING (game total)\n",
    "# --------------------------------\n",
    "# Build derived intensity features to improve total prediction.\n",
    "df_total = df.copy()\n",
    "df_total[\"off_sum\"] = df_total[\"off_epa_pp_home\"] + df_total[\"off_epa_pp_away\"]\n",
    "df_total[\"def_sum_allowed\"] = df_total[\"def_epa_pp_home\"] + df_total[\"def_epa_pp_away\"]\n",
    "df_total[\"to_sum\"] = df_total[\"turnover_rate_home\"] + df_total[\"turnover_rate_away\"]\n",
    "df_total[\"abs_elo_diff\"] = df_total[\"elo_diff\"].abs()\n",
    "\n",
    "train_t = df_total[df_total[\"season\"] <= 2023].copy()\n",
    "test_t = df_total[df_total[\"season\"] >= 2024].copy()\n",
    "\n",
    "feat_total = [\n",
    "    \"off_epa_pp_home\", \"off_epa_pp_away\",\n",
    "    \"def_epa_pp_home\", \"def_epa_pp_away\",\n",
    "    \"turnover_rate_home\", \"turnover_rate_away\",\n",
    "    \"off_sum\", \"def_sum_allowed\", \"to_sum\", \"abs_elo_diff\",\n",
    "    \"week\", \"div_game\", \"rest_diff\",\n",
    "    \"is_dome\", \"is_outdoors\", \"is_retractable\", \"is_grass\",\n",
    "]\n",
    "\n",
    "Xtr_t, ytr_t = train_t[feat_total], train_t[\"total_points\"]\n",
    "Xte_t, yte_t = test_t[feat_total], test_t[\"total_points\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c1fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL Ridge | MAE: 10.365 RMSE: 13.167\n",
      "TOTAL Baseline(mean) | MAE: 10.573 RMSE: 13.524\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Ridge for total points.\n",
    "total_ridge_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge(alpha=5.0)),\n",
    "])\n",
    "\n",
    "total_ridge_model.fit(Xtr_t, ytr_t)\n",
    "pred_t = total_ridge_model.predict(Xte_t)\n",
    "\n",
    "print(\n",
    "    \"TOTAL Ridge | MAE:\",\n",
    "    round(mean_absolute_error(yte_t, pred_t), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_t, pred_t) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Baseline for total: train mean.\n",
    "pred_t_base = np.full(len(test_t), train_t[\"total_points\"].mean())\n",
    "print(\n",
    "    \"TOTAL Baseline(mean) | MAE:\",\n",
    "    round(mean_absolute_error(yte_t, pred_t_base), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_t, pred_t_base) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Conclusion: Ridge beats the mean baseline for total points (MAE 10.365 vs 10.573) with modest but consistent gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b60a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL HGB | MAE: 11.539 RMSE: 14.568\n"
     ]
    }
   ],
   "source": [
    "# Optional nonlinear benchmark for total points.\n",
    "total_hgb_model = HistGradientBoostingRegressor(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=1200,\n",
    "    min_samples_leaf=20,\n",
    "    l2_regularization=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "total_hgb_model.fit(Xtr_t, ytr_t)\n",
    "pred_t_hgb = total_hgb_model.predict(Xte_t)\n",
    "\n",
    "print(\n",
    "    \"TOTAL HGB | MAE:\",\n",
    "    round(mean_absolute_error(yte_t, pred_t_hgb), 3),\n",
    "    \"RMSE:\",\n",
    "    round(mean_squared_error(yte_t, pred_t_hgb) ** 0.5, 3),\n",
    ")\n",
    "\n",
    "# Conclusion: HGB is worse for total points (MAE 11.539), indicating overfit/poor generalization vs Ridge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ecc3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit A (elo_diff) | AUC=0.7113  LogLoss=0.6240  Acc=0.6555\n",
      "Logit B (+EPA/+TO) | AUC=0.7147  LogLoss=0.6186  Acc=0.6626\n",
      "Logit C (+roof/surface) | AUC=0.7104  LogLoss=0.6205  Acc=0.6608\n",
      "Baseline coinflip | LogLoss= 0.6931  AUC=0.5\n",
      "Baseline base-rate | LogLoss= 0.6899\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# HOME WIN PROBABILITY MODELING (P[home_win])\n",
    "# -----------------------------------\n",
    "# Use logistic regression to predict calibrated win probabilities.\n",
    "train_w = df[df[\"season\"] <= 2023].copy()\n",
    "test_w = df[df[\"season\"] >= 2024].copy()\n",
    "\n",
    "ytr_w = train_w[\"home_win\"].astype(int)\n",
    "yte_w = test_w[\"home_win\"].astype(int)\n",
    "\n",
    "\n",
    "def fit_eval_logit(Xtr, Xte, ytr, yte, name):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logit\", LogisticRegression(C=1.0, solver=\"lbfgs\", max_iter=2000)),\n",
    "    ])\n",
    "    model.fit(Xtr, ytr)\n",
    "    p = model.predict_proba(Xte)[:, 1]\n",
    "    pred = (p >= 0.5).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(yte, p)\n",
    "    ll = log_loss(yte, p)\n",
    "    acc = accuracy_score(yte, pred)\n",
    "\n",
    "    print(f\"{name} | AUC={auc:.4f}  LogLoss={ll:.4f}  Acc={acc:.4f}\")\n",
    "    return model, p\n",
    "\n",
    "# Model A: Elo only\n",
    "feat_A = [\"elo_diff\"]\n",
    "model_A, pA = fit_eval_logit(train_w[feat_A], test_w[feat_A], ytr_w, yte_w, \"Logit A (elo_diff)\")\n",
    "\n",
    "# Model B: Elo + performance differentials\n",
    "feat_B = [\"elo_diff\", \"off_diff\", \"def_allowed_diff\", \"to_diff\", \"rest_diff\", \"div_game\"]\n",
    "model_B, pB = fit_eval_logit(train_w[feat_B], test_w[feat_B], ytr_w, yte_w, \"Logit B (+EPA/+TO)\")\n",
    "\n",
    "# Model C: add environment flags\n",
    "feat_C = feat_B + [\"is_dome\", \"is_outdoors\", \"is_retractable\", \"is_grass\"]\n",
    "model_C, pC = fit_eval_logit(train_w[feat_C], test_w[feat_C], ytr_w, yte_w, \"Logit C (+roof/surface)\")\n",
    "\n",
    "# Probability baselines for context.\n",
    "p_coinflip = np.full(len(test_w), 0.5)\n",
    "p_base_rate = np.full(len(test_w), ytr_w.mean())\n",
    "print(\"Baseline coinflip | LogLoss=\", round(log_loss(yte_w, p_coinflip), 4), \" AUC=0.5\")\n",
    "print(\"Baseline base-rate | LogLoss=\", round(log_loss(yte_w, p_base_rate), 4))\n",
    "\n",
    "# Conclusion: Logit B is the best classifier (AUC 0.7147, LogLoss 0.6186), while roof/surface adds no net gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37092e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_diff            -0.021294\n",
      "div_game           -0.014512\n",
      "def_allowed_diff    0.026996\n",
      "rest_diff           0.089483\n",
      "off_diff            0.243318\n",
      "elo_diff            0.547090\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Inspect coefficients from Model B to understand directional effects.\n",
    "logit_B = model_B.named_steps[\"logit\"]\n",
    "coef_B = pd.Series(logit_B.coef_[0], index=feat_B).sort_values()\n",
    "print(coef_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98143f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEA @ NE\n",
      "Pred home win prob: 0.436  (pick: AWAY)\n",
      "Pred spread (home margin): -0.81  -> suggested line: NE 0.81\n",
      "Pred total points: 43.44\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# SINGLE-GAME INFERENCE EXAMPLE\n",
    "# -----------------------------------\n",
    "# Load schedule features and pick one upcoming game row (usually the latest row).\n",
    "df_pred = pd.read_parquet(\"../data/schedule_features_2016_2025.parquet\").tail(1).copy()\n",
    "\n",
    "X = df_pred.copy()\n",
    "\n",
    "# Recreate the same engineered features used during training.\n",
    "X[\"elo_diff\"] = X[\"r_home\"] - X[\"r_away\"]\n",
    "X[\"off_diff\"] = X[\"off_epa_pp_home\"] - X[\"off_epa_pp_away\"]\n",
    "X[\"def_allowed_diff\"] = X[\"def_epa_pp_away\"] - X[\"def_epa_pp_home\"]\n",
    "X[\"to_diff\"] = X[\"turnover_rate_away\"] - X[\"turnover_rate_home\"]\n",
    "X[\"rest_diff\"] = X[\"home_rest\"] - X[\"away_rest\"]\n",
    "\n",
    "X[\"roof\"] = X[\"roof\"].astype(str).str.strip().str.lower()\n",
    "X[\"surface\"] = X[\"surface\"].astype(str).str.strip().str.lower()\n",
    "X[\"is_grass\"] = (X[\"surface\"] == \"grass\").astype(int)\n",
    "X[\"is_dome\"] = (X[\"roof\"] == \"dome\").astype(int)\n",
    "X[\"is_outdoors\"] = (X[\"roof\"] == \"outdoors\").astype(int)\n",
    "X[\"is_retractable\"] = X[\"roof\"].isin([\"open\", \"closed\"]).astype(int)\n",
    "\n",
    "X[\"off_sum\"] = X[\"off_epa_pp_home\"] + X[\"off_epa_pp_away\"]\n",
    "X[\"def_sum_allowed\"] = X[\"def_epa_pp_home\"] + X[\"def_epa_pp_away\"]\n",
    "X[\"to_sum\"] = X[\"turnover_rate_home\"] + X[\"turnover_rate_away\"]\n",
    "X[\"abs_elo_diff\"] = X[\"elo_diff\"].abs()\n",
    "\n",
    "home = X.iloc[0][\"home_team\"]\n",
    "away = X.iloc[0][\"away_team\"]\n",
    "\n",
    "# Predict spread (home margin).\n",
    "pred_margin = spread_model.predict(X[feat_spread])[0]\n",
    "\n",
    "# Predict total points.\n",
    "pred_total = total_ridge_model.predict(X[feat_total])[0]\n",
    "\n",
    "# Predict home win probability.\n",
    "p_home_win = model_B.predict_proba(X[feat_B])[:, 1][0]\n",
    "pick_home = int(p_home_win >= 0.5)\n",
    "\n",
    "print(f\"{away} @ {home}\")\n",
    "print(f\"Pred home win prob: {p_home_win:.3f}  (pick: {'HOME' if pick_home else 'AWAY'})\")\n",
    "print(f\"Pred spread (home margin): {pred_margin:.2f}  -> suggested line: {home} {(-pred_margin):.2f}\")\n",
    "print(f\"Pred total points: {pred_total:.2f}\")\n",
    "\n",
    "# Conclusion: For SEA 29 - NE 13, the model correctly picked AWAY (SEA), estimated total close (43.44 vs 42), but strongly underestimated spread magnitude (-0.81 vs -16).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af493801-71b6-47ea-9087-22756d8fbf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
